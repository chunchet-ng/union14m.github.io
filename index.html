<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="An Introduction page to Union14M dataset and MAERec.">
  <meta name="keywords" content="STR, Union14M, MAERec">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Rethinking Scene Text Recognition: A Data Perspective</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Rethink Scene Text Recognition: A Data Perspective</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/Mountchicken">Qing Jiang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/jpWang">Jiapeng Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/shannanyinxiang">Dezhi Peng</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/lcy0604">Chongyu Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.dlvc-lab.net/lianwen/">Lianwen Jin</a><sup>1</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>South China University of Technology</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="material/Revisiting_Scene_Text_Recognition__A_Data_Perspective__arXiv_.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- arxiv Link. -->
                <span class="link-block">
                  <a href="material/Revisiting_Scene_Text_Recognition__A_Data_Perspective__arXiv_.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/Mountchicken/Union14M"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Demo Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/Mountchicken/MAERec-Gradio"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/Mountchicken/Union14M#34-download"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/union14ml.png" style="width: 100%" alt>
        <img src="static/images/union14mu.png" style="width: 100%" alt>
        <h2 class="subtitle has-text-centered">
          <a href="https://github.com/Mountchicken/Union14M" target="_blank" title="tag_list"><span
              style="color: pink; font-weight:bold">Union14M contains 4M labeled data and 10M Unlabeled data.</span></a>
        </h2>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Two Questions for Scene Text Recognition</h2>
          <div class="content has-text-justified">
            <p>
              Scene Text Recognition (STR) is a fundamental task in computer vision, which aims to recognize the text in
              natural
              images. STR has been developed rapidly, and recent state-of-the-arts have shown a trend of
              accuracy
              saturation on six commonly used benchmarks (IC13, IC15, SVT, IIIT5K, SVTP, CUTE80).
            </p>
            <p>
            <div align=center>
              <img src="static/images/benchmark_acc.png" style="width: 60%" alt>
            </div>
            </p>
            <p>
              The challenges in the common benchmarks seem “solved”,
              suggested by the narrow scope for improvement, and the slowdown step of performance
              gain in recent SOTAs. This phenomenon inspires us to raise questions
              of <b>1) whether the common benchmarks remain sufficient to promote future progress</b>
              and <b>2) whether this accuracy saturation implies that STR is solved.</b>
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">

        <!-- Visual Effects. -->
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Q1: Are common benchmarks remain challenging?</h2>
            <p>
              we start by selecting 13 representative models, including CTC-based
              attention-based and language model-based models.
              We evaluate their performance on the six STR benchmarks to find their joint errors. As
              depicted below, only 3.9% (298 images) of the
              total 7672 benchmark images can not be correctly
              recognized by any models. And there might be only 1.53% scope for
              accuracy improvement. Therefore, the common benchmarks give limited insight into future STR research.
            </p>
            <p>
            <div align=center>
              <img src="static/images/error.png" style="width: 100%" alt>
            </div>
            </p>
          </div>
        </div>
        <!--/ Visual Effects. -->

        <!-- Matting. -->
        <div class="column">
          <h2 class="title is-3">Q2: Is STR solved or challenges are obscured?</h2>
          <div class="columns is-centered">
            <div class="column content">
              <p>
                The accuracy saturation in common benchmarks can obscure challenges that STR models
                still face. Therefore, we build Union14M to bring more profound insights beyond these benchmarks.
                We discover that STR models perform poorly on Union14M-L, with an average accuracy of only 66.53%,
                despite achieving an average accuracy of 87.03% on commonly used benchmarks. This suggest that
                STR is far from being solved.
              </p>
              <p>
              <div align=center>
                <img src="static/images/performance.png" style="width: 100%" alt>
              </div>
              </p>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Union14M: A Large-Scale STR Dataset</h2>
          <p>
            Union14M consists of 4
            million <b>L</b>abeled images (Union14M-L) and 10 million <b>U</b>nlabeled
            images (Union14M-U), obtained from 17 publicly available datasets. It can
            be considered as a comprehensive representation of text images in the real world.
          </p>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">

        <!-- Visual Effects. -->
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Union14M-L: 4M Labeled Real Data</h2>
            <p>
              Union14M-L contains 4M images collected from 14 public available datasets. We adopt serval strategies
              to refine the naive concatation of the 14 datasaets, including cropping (We use minimal axis-aligned
              bounding box to
              crop the images)
              and de-duplication (Some datasets contains duplicate images and we remove them). We also categorize the
              images in Union14M-L into five difficulty levels using an error voting method.
            </p>
            <p>
            <div align=center>
              <img src="static/images/union14ml.png" style="width: 100%" alt>
            </div>
          </div>
        </div>
        <!--/ Visual Effects. -->

        <!-- Matting. -->
        <div class="column">
          <h2 class="title is-3">Union14M-U: 10M Unlabeled Real Data</h2>
          <div class="columns is-centered">
            <div class="column content">
              <p>
                The optimal solution to improve the performance of STR in real-world scenarios is to utilize more data
                for training.
                However, labeling text images is both costly and time-intensive, given that it involves annotating
                sequences and needs
                specialized language expertise. Therefore, it would be desirable to investigate the potential of
                utilizing unlabeled
                data via self-supervised learning for STR. To this end we collect 10M unlabeled images from 3 large
                datasets with an
                IoU Voting method
              </p>
              <div align=center>
                <img src="static/images/union14mu.png" style="width: 100%" alt>
              </div>
            </div>

          </div>
        </div>
      </div>
      <!--/ Matting. -->

      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Union14M-Benchmark</h2>
          <div class="content has-text-justified">
            <p>
              We raise seven open challenges for STR in real-world scenarios, and propose a challenge-driven benchmark
              to facilitate
              the future development.
            </p>
            <div align=center>
              <img src="static/images/errorany.png" style="width: 80%" alt>
            </div>
          </div>
        </div>
      </div>
      <!--/ Animation. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">MAERec: A Real-world Adapted Recognizer</h2>
          <p>
            MAERec is a scene text recognition model composed of a ViT backbone and a Transformer decoder in
            auto-regressive style.
            It shows an outstanding performance in scene text recognition, especially when pre-trained on the Union14M-U
            through
            MAE.
          </p>
          <div align=center>
            <img src="static/images/maerec.png" style="width: 80%" alt>
          </div>
        </div>
      </div>
  </section>



  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2303.05657">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/Mountchicken/Union14M" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="content">
          <p>
            The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
